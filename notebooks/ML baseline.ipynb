{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_fscore_support\n",
    "\n",
    "# --- 1. Завантаження даних ---\n",
    "print(\"--- 1. Data Loading ---\")\n",
    "\n",
    "train_df_raw = pd.read_csv('../data/liar/train_filtered.csv')\n",
    "val_df_raw = pd.read_csv('../data/liar/valid_filtered.csv')\n",
    "test_df_raw = pd.read_csv('../data/liar/test_filtered.csv')\n",
    "\n",
    "#\n",
    "# train_df_raw = pd.read_csv('../data/fakenewsnet_dataset/combined_train.csv')\n",
    "# val_df_raw = pd.read_csv('../data/fakenewsnet_dataset/combined_val.csv')\n",
    "# test_df_raw = pd.read_csv('../data/fakenewsnet_dataset/combined_test.csv')\n",
    "\n",
    "# --- 2. Очищення даних та синхронізація X та y ---\n",
    "print(\"\\n--- 2. Data Cleaning and X, y Synchronization ---\")\n",
    "\n",
    "def clean_and_prepare_data(df, text_column='statement', label_column='binary_label'):\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    # Перевірка на NaN у текстовому стовпці та видалення\n",
    "    initial_nan_count = df[text_column].isna().sum()\n",
    "    if initial_nan_count > 0:\n",
    "        print(f\"NaNs found in '{text_column}': {initial_nan_count}\")\n",
    "        df.dropna(subset=[text_column], inplace=True)\n",
    "        print(f\"Shape after dropping NaNs from '{text_column}': {df.shape}\")\n",
    "\n",
    "    # Видалення рядків, де текст порожній або складається лише з пробілів\n",
    "    initial_len = len(df)\n",
    "    # Використовуємо .loc для уникнення SettingWithCopyWarning та для коректного відбору\n",
    "    df = df.loc[df[text_column].apply(lambda x: isinstance(x, str) and x.strip() != '')].copy()\n",
    "    empty_removed_count = initial_len - len(df)\n",
    "    if empty_removed_count > 0:\n",
    "        print(f\"Empty/whitespace-only strings removed from '{text_column}': {empty_removed_count}\")\n",
    "        print(f\"Shape after removing empty strings: {df.shape}\")\n",
    "\n",
    "    # Перевірка на NaN у цільовому стовпці (якщо потрібно, але зазвичай мітки не бувають NaN)\n",
    "    nan_labels = df[label_column].isna().sum()\n",
    "    if nan_labels > 0:\n",
    "        print(f\"Warning: NaNs found in label column '{label_column}': {nan_labels}. Consider how to handle these.\")\n",
    "        # df.dropna(subset=[label_column], inplace=True) # Розкоментуйте, якщо потрібно видаляти\n",
    "        # print(f\"Shape after dropping NaNs from '{label_column}': {df.shape}\")\n",
    "\n",
    "    X = df[text_column]\n",
    "    y = df[label_column]\n",
    "    print(f\"Final shapes: X: {X.shape}, y: {y.shape}\")\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = clean_and_prepare_data(train_df_raw.copy(), text_column='statement', label_column='binary_label')\n",
    "X_val, y_val = clean_and_prepare_data(val_df_raw.copy(), text_column='statement', label_column='binary_label')\n",
    "X_test, y_test = clean_and_prepare_data(test_df_raw.copy(), text_column='statement', label_column='binary_label')\n",
    "\n",
    "print(\"\\nNaN check after cleaning and splitting:\")\n",
    "print(\"NaN in X_train:\", X_train.isna().sum())\n",
    "print(\"NaN in X_val:\", X_val.isna().sum())\n",
    "print(\"NaN in X_test:\", X_test.isna().sum())\n",
    "print(\"NaN in y_train:\", y_train.isna().sum()) # Додаткова перевірка для y\n",
    "print(\"NaN in y_val:\", y_val.isna().sum())\n",
    "print(\"NaN in y_test:\", y_test.isna().sum())\n",
    "\n",
    "\n",
    "# Переконайтеся, що індекси скинуті, якщо це необхідно для подальшої обробки\n",
    "# (хоча для TfidfVectorizer та навчання моделей це зазвичай не критично, якщо X та y вирівняні)\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "print(\"Shape of X_train_vec:\", X_train_vec.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "y_train = y_train[:X_train_vec.shape[0]]\n",
    "print(\"Shape of X_train_vec:\", X_train_vec.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_rf.fit(X_train_vec, y_train)\n",
    "model_xgb.fit(X_train_vec, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, name='Model'):\n",
    "    y_probs = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_probs >= 0.5).astype(int)\n",
    "\n",
    "    print(f\"=== {name} REPORT ===\")\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    # Матриця плутанини\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC-крива\n",
    "    fpr, tpr, _ = roc_curve(y, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{name} ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Розподіл “невпевнених” прогнозів\n",
    "    uncertain = (y_probs >= 0.4) & (y_probs <= 0.6)\n",
    "    print(f\"{name} uncertain cases (prob 0.4–0.6): {np.sum(uncertain)} / {len(y)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_test = y_test[:X_test_vec.shape[0]]\n",
    "evaluate_model(model_rf, X_test_vec, y_test, name='Random Forest')\n",
    "evaluate_model(model_xgb, X_test_vec, y_test, name='XGBoost')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for model, name in [(model_rf, 'Random Forest'), (model_xgb, 'XGBoost')]:\n",
    "    for X, y, ds_name in [(X_train_vec, y_train, 'Train'),\n",
    "                          (X_val_vec, y_val, 'Validation'),\n",
    "                          (X_test_vec, y_test, 'Test')]:\n",
    "\n",
    "        y_probs = model.predict_proba(X)[:, 1]\n",
    "        y_pred = (y_probs >= 0.5).astype(int)\n",
    "        y = y[:y_pred.shape[0]]\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y, y_pred, average='binary')\n",
    "        acc = np.mean(y_pred == y)\n",
    "        fpr, tpr, _ = roc_curve(y, y_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        uncertain = np.sum((y_probs >= 0.4) & (y_probs <= 0.6)) / len(y)\n",
    "\n",
    "        all_results.append({\n",
    "            'Model': name,\n",
    "            'Dataset': ds_name,\n",
    "            'Accuracy': acc,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1,\n",
    "            'AUC': roc_auc,\n",
    "            'Uncertain (%)': uncertain * 100\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df)\n",
    "results_df.to_csv('../results/model_results.csv', index=False)\n",
    "print(\"Saved results to ../results/model_results.csv\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Функція оцінки\n",
    "def evaluate_model(model, X, y, dataset_name, model_name):\n",
    "    y_probs = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_probs >= 0.5).astype(int)\n",
    "    y = y[:y_pred.shape[0]]\n",
    "    print(f\"=== {model_name} on {dataset_name} ===\")\n",
    "    report = classification_report(y, y_pred, output_dict=True)\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "    # Матриця плутанини\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} {dataset_name} Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC-крива\n",
    "    fpr, tpr, _ = roc_curve(y, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{dataset_name} (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} {dataset_name} ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Невпевнені прогнози\n",
    "    uncertain = (y_probs >= 0.4) & (y_probs <= 0.6)\n",
    "    print(f\"{model_name} {dataset_name} uncertain cases (prob 0.4–0.6): {np.sum(uncertain)} / {len(y)}\")\n",
    "\n",
    "    return report\n",
    "\n",
    "# Порівняння train / val / test\n",
    "for model, name in [(model_rf, 'Random Forest'), (model_xgb, 'XGBoost')]:\n",
    "    print(f\"\\n\\n==== {name} ANALYSIS ====\")\n",
    "    evaluate_model(model, X_train_vec, y_train, 'Train', name)\n",
    "    evaluate_model(model, X_val_vec, y_val, 'Validation', name)\n",
    "    evaluate_model(model, X_test_vec, y_test, 'Test', name)\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def plot_pr_curve(y_true, y_probs, model_name, ds_name):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{model_name} {ds_name} Precision-Recall Curve')\n",
    "    plt.show()\n",
    "\n",
    "for model, name in [(model_rf, 'Random Forest'), (model_xgb, 'XGBoost')]:\n",
    "    for X, y, ds_name in [(X_test_vec, y_test, 'Test')]:\n",
    "        y_probs = model.predict_proba(X)[:, 1]\n",
    "        plot_pr_curve(y, y_probs, name, ds_name)\n",
    "\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "calibrated_rf = CalibratedClassifierCV(model_rf, method='isotonic', cv='prefit')\n",
    "\n",
    "calibrated_rf.fit(X_val_vec, y_val)\n",
    "\n",
    "y_probs_calib = calibrated_rf.predict_proba(X_test_vec)[:, 1]\n",
    "brier = brier_score_loss(y_test, y_probs_calib)\n",
    "print(f'Brier score (calibrated Random Forest on Test): {brier:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Важливість ознак\n",
    "def plot_feature_importance(model, vectorizer, model_name, top_n=20):\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        importances = model.get_booster().get_score(importance_type='weight')\n",
    "        feature_names = np.array(list(importances.keys()))\n",
    "        importances = np.array(list(importances.values()))\n",
    "        importances = importances / importances.sum()\n",
    "\n",
    "    sorted_idx = np.argsort(importances)[::-1][:top_n]\n",
    "    top_features = feature_names[sorted_idx]\n",
    "    top_importances = importances[sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_importances, y=top_features)\n",
    "    plt.title(f'{model_name} Top {top_n} Important Features')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "plot_feature_importance(model_rf, vectorizer, 'Random Forest', top_n=20)\n",
    "plot_feature_importance(model_xgb, vectorizer, 'XGBoost', top_n=20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
